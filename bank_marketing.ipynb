{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os.path\n",
    "\n",
    "import joblib\n",
    "import mlflow\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    confusion_matrix,\n",
    "    RocCurveDisplay,\n",
    "    ConfusionMatrixDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    PolynomialFeatures,\n",
    ")\n",
    "from warnings import filterwarnings\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the business\n",
    "\n",
    "It comes as no surprise that large marketing campaigns have negative sentiment amongst the general populace. Think about the last time you answered an unexpected phone call from an unknown number, if your experience is anything like my then, it was either a scam caller, telemarketer or survey taker. I find myself hanging up quickly when it comes to these types of calls, if I were to ever answer them. Every failed cold call costs the company commissioning the campaign time and money. The bank partner commissioning this study is seeking to increase campaign success and reduce costs by focusing on profiles that are more likely to accept their offerings. The bank partner would like a model that can better predict the type of person that would accept offers from our partner bank."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the Features\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Understanding the Data\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"data/bank-additional-full.csv\", sep=\";\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe(include=\"object\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Only 12 duplicates\n",
    "print(\n",
    "    f\"Row count: {df.shape[0]}, Duplicate count: {df.shape[0] - df.drop_duplicates().shape[0]}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# No columns missing data\n",
    "df.isna().mean().round(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.sample(5)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_counts = df[\"y\"].value_counts()\n",
    "fig = px.bar(\n",
    "    y_counts,\n",
    "    y=\"count\",\n",
    "    title=\"Those that say yes are in a minority class, suggesting an imbalanced dataset\",\n",
    "    labels={\"count\": \"Count\", \"y\": \"Accepted Campaign\"},\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/acceptance_count.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.box(\n",
    "    df,\n",
    "    y=\"age\",\n",
    "    title=\"Most potential calls are towards people aged 32-47\",\n",
    "    labels={\"age\": \"Age\"},\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/age_box.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store commonly used group by\n",
    "by_y_df = df.groupby(\"y\")\n",
    "by_y_df.describe(include=\"object\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ratio calculation, this should be a better metric to track over counts\n",
    "age_ratio = (by_y_df[\"age\"].value_counts() / df[\"age\"].value_counts()).reset_index()\n",
    "fig = px.bar(\n",
    "    age_ratio.sort_values(by=\"count\"),\n",
    "    x=\"age\",\n",
    "    y=\"count\",\n",
    "    color=\"y\",\n",
    "    title=\"Older than 60 and younger than 23 gives at least a 20% success rate\",\n",
    "    labels={\n",
    "        \"age\": \"Age\",\n",
    "        \"count\": \"Ratio accepting campaign\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/age_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "job_ratio = (by_y_df[\"job\"].value_counts() / df[\"job\"].value_counts()).reset_index()\n",
    "fig = px.bar(\n",
    "    job_ratio.sort_values(by=\"count\"),\n",
    "    x=\"job\",\n",
    "    y=\"count\",\n",
    "    color=\"y\",\n",
    "    title=\"Students and retirees are more likely to accept campaign\",\n",
    "    labels={\n",
    "        \"job\": \"Job\",\n",
    "        \"count\": \"Ratio accepting campaign\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/job_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "education_ratio = (\n",
    "    by_y_df[\"education\"].value_counts() / df[\"education\"].value_counts()\n",
    ").reset_index()\n",
    "fig = px.bar(\n",
    "    education_ratio.sort_values(by=\"count\"),\n",
    "    x=\"education\",\n",
    "    y=\"count\",\n",
    "    color=\"y\",\n",
    "    title=\"People who are illiterate are more likely to accept campaign\",\n",
    "    labels={\n",
    "        \"education\": \"Education\",\n",
    "        \"count\": \"Ratio accepting campaign\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/education_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "month_ratio = (\n",
    "    by_y_df[\"month\"].value_counts() / df[\"month\"].value_counts()\n",
    ").reset_index()\n",
    "fig = px.bar(\n",
    "    month_ratio.sort_values(by=\"count\"),\n",
    "    x=\"month\",\n",
    "    y=\"count\",\n",
    "    color=\"y\",\n",
    "    title=\"Month has a strong effect on acceptance\",\n",
    "    labels={\n",
    "        \"month\": \"Month\",\n",
    "        \"count\": \"Ratio accepting campaign\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/month_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x=\"duration\",\n",
    "    color=\"y\",\n",
    "    title=\"The longer a call goes the higher the likelihood of acceptance\",\n",
    "    labels={\"duration\": \"Duration\", \"y\": \"Accepted Campaign\"},\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/duration_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    df,\n",
    "    x=\"campaign\",\n",
    "    color=\"y\",\n",
    "    title=\"There are diminishing returns when making a call to the same person\",\n",
    "    labels={\"campaign\": \"Number of calls\", \"y\": \"Accepted Campaign\"},\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/calls_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    df[df[\"pdays\"] != 999],\n",
    "    x=\"pdays\",\n",
    "    color=\"y\",\n",
    "    title=\"Following up within a week increases the chance of success\",\n",
    "    labels={\n",
    "        \"pdays\": \"Days since last contact\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "        \"count\": \"Count\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/days_since_last_contact_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correlation plots\n",
    "corr_matrix = df.corr(numeric_only=True).round(2)\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    title=\"Unsurprisingly, social and economic attributes highly correlated\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "fig.update_layout(height=1000, width=1000, showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(\"images/correlation.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    by_y_df[\"cons.price.idx\"].value_counts().reset_index(),\n",
    "    x=\"cons.price.idx\",\n",
    "    y=\"count\",\n",
    "    color=\"y\",\n",
    "    title=\"Number of acceptors look static across consumer price index\",\n",
    "    labels={\n",
    "        \"cons.price.idx\": \"Consumer price index\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "        \"count\": \"Count\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/consumer_price_index_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    by_y_df[\"cons.conf.idx\"].value_counts().reset_index(),\n",
    "    x=\"cons.conf.idx\",\n",
    "    y=\"count\",\n",
    "    color=\"y\",\n",
    "    title=\"Consumer confidence index may be used to improve acceptance\",\n",
    "    labels={\n",
    "        \"cons.conf.idx\": \"Consumer confidence index\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "        \"count\": \"Count\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/consumer_confidence_index_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "poutcome_ratio = (\n",
    "    by_y_df[\"poutcome\"].value_counts() / df[\"poutcome\"].value_counts()\n",
    ").reset_index()\n",
    "\n",
    "fig = px.bar(\n",
    "    poutcome_ratio,\n",
    "    x=\"poutcome\",\n",
    "    y=\"count\",\n",
    "    color=\"y\",\n",
    "    title=\"Those that accept the previous campaign are more likely to accept the next campaign\",\n",
    "    labels={\n",
    "        \"poutcome\": \"Previous outcome\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "        \"count\": \"Ratio accepting campaign\",\n",
    "    },\n",
    ")\n",
    "fig.update_layout(height=800, width=1000)\n",
    "fig.show()\n",
    "fig.write_image(\"images/poutcome_acceptance_ratio.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.scatter(\n",
    "    df[df[\"duration\"] != 0],\n",
    "    x=\"duration\",\n",
    "    y=\"campaign\",\n",
    "    color=\"y\",\n",
    "    title=\"Number of calls does not correlate with duration of the call\",\n",
    "    labels={\n",
    "        \"campaign\": \"Number of calls\",\n",
    "        \"duration\": \"Duration\",\n",
    "        \"y\": \"Accepted Campaign\",\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/calls_vs_duration.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the data\n",
    "The first thing that jumps out is how imbalanced the dataset is. This is to be expected considering that we are working with telemarketing data. I was also able to find some strong predictors for the accepting class. In particular, the following fields show strong promise: month, employment, number of contacts. It is also important to note that the pdays column using 999 to signify that fact the client was not priorly contacted. The author of this dataset also recommends avoiding the duration column as it highly affects the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Data Preparation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove outliers\n",
    "q1 = df[\"age\"].quantile(0.25)\n",
    "q3 = df[\"age\"].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - (1.5 * iqr)\n",
    "upper_bound = q3 + (1.5 * iqr)\n",
    "df = df.query(f\"age > 0 and age >= {lower_bound} and age <= {upper_bound}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.drop(\"duration\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.drop_duplicates(inplace=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Enable MLflow's automatic experiment tracking for scikit-learn. This will help with tracking experiments\n",
    "mlflow.sklearn.autolog()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            make_column_selector(dtype_include=np.object_),\n",
    "        ),\n",
    "        (\n",
    "            \"poly\",\n",
    "            PolynomialFeatures(include_bias=False),\n",
    "            make_column_selector(dtype_include=np.number),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split data\n",
    "X = df.drop(\"y\", axis=1)\n",
    "y = df[\"y\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def eval_model(pipe, model_name, key, X_train, X_test, y_train, y_test, best_params):\n",
    "    yes = \"yes\"\n",
    "    train_accuracy = accuracy_score(pipe.predict(X_train), y_train)\n",
    "    train_precision = precision_score(pipe.predict(X_train), y_train, pos_label=yes)\n",
    "    train_recall = recall_score(pipe.predict(X_train), y_train, pos_label=yes)\n",
    "    train_f1 = f1_score(pipe.predict(X_train), y_train, pos_label=yes)\n",
    "\n",
    "    test_accuracy = accuracy_score(pipe.predict(X_test), y_test)\n",
    "    test_precision = precision_score(pipe.predict(X_test), y_test, pos_label=yes)\n",
    "    test_recall = recall_score(pipe.predict(X_test), y_test, pos_label=yes)\n",
    "    test_f1 = f1_score(pipe.predict(X_test), y_test, pos_label=yes)\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"train_precision\": train_precision,\n",
    "        \"test_precision\": test_precision,\n",
    "        \"train_recall\": train_recall,\n",
    "        \"test_recall\": test_recall,\n",
    "        \"train_f1\": train_f1,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"coef\": (\n",
    "            pipe.named_steps[key].coef_\n",
    "            if hasattr(pipe, 'named_steps') and hasattr(pipe.named_steps[key], \"coef_\")\n",
    "            else None\n",
    "        ),\n",
    "        \"best_params\": best_params,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "precision_scorer = make_scorer(precision_score, pos_label=\"yes\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scores = []",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dummy_evaluation = eval_model(\n",
    "    dummy_clf,\n",
    "    \"DummyClassifier\",\n",
    "    \"dummy\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    {},\n",
    ")\n",
    "scores.append(dummy_evaluation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tree_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"smote__k_neighbors\": [3, 5],\n",
    "    \"smote__sampling_strategy\": [0.3, 0.5],\n",
    "    \"tree__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"tree__max_depth\": [3, 4, 5],\n",
    "    \"tree__min_samples_split\": [4, 5],\n",
    "    \"tree__min_samples_leaf\": [3, 4, 5],\n",
    "    \"preprocessor__poly__degree\": [2, 3],\n",
    "}\n",
    "\n",
    "grid_tree = GridSearchCV(tree_pipe, param_grid=params, scoring=precision_scorer, verbose=1)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "print(grid_tree.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tree_evaluation = eval_model(\n",
    "    grid_tree.best_estimator_,\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"tree\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_tree.best_params_,\n",
    ")\n",
    "scores.append(tree_evaluation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joblib.dump(grid_tree.best_estimator_, \"bank_marketing_decision_tree.pkl\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"lr\", LogisticRegression(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"smote__k_neighbors\": [3, 5],\n",
    "    \"smote__sampling_strategy\": [0.3, 0.5],\n",
    "    \"lr__penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"lr__max_iter\": [500, 1000],\n",
    "    \"lr__C\": [0.01, 0.1, 1],\n",
    "    \"preprocessor__poly__degree\": [2, 3],\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(lr_pipe, param_grid=params, scoring=precision_scorer, verbose=1)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "print(grid_lr.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr_evaluation = eval_model(\n",
    "    grid_lr.best_estimator_,\n",
    "    \"LogisticRegression\",\n",
    "    \"lr\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_lr.best_params_,\n",
    ")\n",
    "scores.append(lr_evaluation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joblib.dump(grid_lr.best_estimator_, \"bank_marketing_logistic_regression.pkl\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "knn_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"knn\", KNeighborsClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"smote__k_neighbors\": [3, 5],\n",
    "    \"smote__sampling_strategy\": [0.3, 0.5],\n",
    "    \"knn__n_neighbors\": [3, 5],\n",
    "    \"preprocessor__poly__degree\": [2, 3],\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(knn_pipe, param_grid=params, scoring=precision_scorer, verbose=1)\n",
    "grid_knn.fit(X_train, y_train)\n",
    "print(grid_knn.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "knn_evaluation = eval_model(\n",
    "    grid_knn.best_estimator_,\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"knn\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_knn.best_params_,\n",
    ")\n",
    "scores.append(knn_evaluation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joblib.dump(grid_knn.best_estimator_, \"bank_marketing_knn.pkl\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "svc_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"svc\", SVC(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"smote__k_neighbors\": [3, 5],\n",
    "    \"smote__sampling_strategy\": [0.3, 0.5],\n",
    "    \"svc__C\": [0.01, 0.1, 1],\n",
    "    \"preprocessor__poly__degree\": [2, 3],\n",
    "}\n",
    "\n",
    "grid_svc = GridSearchCV(svc_pipe, param_grid=params, scoring=precision_scorer, verbose=1)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "print(grid_svc.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "svc_evaluation = eval_model(\n",
    "    grid_svc.best_estimator_,\n",
    "    \"SVC\",\n",
    "    \"svc\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_svc.best_params_,\n",
    ")\n",
    "scores.append(svc_evaluation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joblib.dump(grid_svc.best_estimator_, \"bank_marketing_svc.pkl\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forest_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"forest\", RandomForestClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"smote__k_neighbors\": [3, 5],\n",
    "    \"smote__sampling_strategy\": [0.3, 0.5],\n",
    "    \"forest__n_estimators\": [100, 200],\n",
    "    \"forest__max_depth\": [3, 4, 5],\n",
    "    \"forest__min_samples_split\": [4, 5],\n",
    "    \"forest__min_samples_leaf\": [3, 4, 5],\n",
    "    \"preprocessor__poly__degree\": [2, 3],\n",
    "}\n",
    "\n",
    "grid_forest = GridSearchCV(\n",
    "    forest_pipe, param_grid=params, scoring=precision_scorer, verbose=1\n",
    ")\n",
    "grid_forest.fit(X_train, y_train)\n",
    "print(grid_forest.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forest_evaluation = eval_model(\n",
    "    grid_forest.best_estimator_,\n",
    "    \"RandomForestClassifier\",\n",
    "    \"forest\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_forest.best_params_,\n",
    ")\n",
    "scores.append(forest_evaluation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joblib.dump(grid_forest.best_estimator_, \"bank_marketing_forest.pkl\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df.to_csv(\"data/scores_df.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_scores_data():\n",
    "    if os.path.exists(\"data/scores_df.csv\"):\n",
    "        return pd.read_csv(\"data/scores_df.csv\")\n",
    "    print(\"Scores csv doesn't exist, train and score models first\")\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Numeric columns\n",
    "numeric_columns = [\n",
    "    \"age\",\n",
    "    \"campaign\",\n",
    "    \"pdays\",\n",
    "    \"previous\",\n",
    "    \"emp.var.rate\",\n",
    "    \"cons.price.idx\",\n",
    "    \"cons.conf.idx\",\n",
    "    \"euribor3m\",\n",
    "    \"nr.employed\",\n",
    "]\n",
    "\n",
    "# Non-numeric columns\n",
    "category_columns = [\n",
    "    \"job\",\n",
    "    \"marital\",\n",
    "    \"education\",\n",
    "    \"default\",\n",
    "    \"housing\",\n",
    "    \"loan\",\n",
    "    \"contact\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"poutcome\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_feature_names(grid):\n",
    "    preprocessor = grid.best_estimator_.named_steps[\"preprocessor\"]\n",
    "    scaler_features = preprocessor.named_transformers_[\"scaler\"].get_feature_names_out(\n",
    "        numeric_columns\n",
    "    )\n",
    "    poly_features = preprocessor.named_transformers_[\"poly\"].get_feature_names_out(\n",
    "        numeric_columns\n",
    "    )\n",
    "\n",
    "    onehot_features = preprocessor.named_transformers_[\"onehot\"].get_feature_names_out(\n",
    "        category_columns\n",
    "    )\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            scaler_features,\n",
    "            poly_features,\n",
    "            onehot_features,\n",
    "        ]\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scores_df = load_scores_data()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = make_subplots(\n",
    "    rows=4,\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Train vs Test Accuracy\",\n",
    "        \"Train vs Test Precision\",\n",
    "        \"Train vs Test Recall\",\n",
    "        \"Train vs Test F1\",\n",
    "    ),\n",
    ")\n",
    "metrics = [\n",
    "    (\"train_accuracy\", \"test_accuracy\"),\n",
    "    (\"train_precision\", \"test_precision\"),\n",
    "    (\"train_recall\", \"test_recall\"),\n",
    "    (\"train_f1\", \"test_f1\"),\n",
    "]\n",
    "\n",
    "\n",
    "def title_case(word):\n",
    "    word.replace(\"_\", \" \")\n",
    "    return word.title()\n",
    "\n",
    "\n",
    "for i, metric_tuple in enumerate(metrics):\n",
    "    train_bar = go.Bar(\n",
    "        x=scores_df[\"model_name\"],\n",
    "        y=scores_df[metric_tuple[0]],\n",
    "        name=title_case(metric_tuple[0]),\n",
    "    )\n",
    "    fig.add_trace(train_bar, row=i + 1, col=1)\n",
    "    test_bar = go.Bar(\n",
    "        x=scores_df[\"model_name\"],\n",
    "        y=scores_df[metric_tuple[1]],\n",
    "        name=title_case(metric_tuple[1]),\n",
    "    )\n",
    "    fig.add_trace(test_bar, row=i + 1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Train vs Test Accuracy, Precision, Recall and F1\",\n",
    "    yaxis_title=\"Metric performance\",\n",
    "    height=1000,\n",
    "    width=1800,\n",
    "    showlegend=True,\n",
    "    barmode=\"group\",\n",
    "    bargap=0.35,\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/metric_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Roc Curve Logistic Regression\n",
    "fig, ax = plt.subplots()\n",
    "RocCurveDisplay.from_estimator(\n",
    "    grid_lr, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"LogisticRegression\"\n",
    ")\n",
    "RocCurveDisplay.from_estimator(\n",
    "    grid_knn, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"KNN\"\n",
    ")\n",
    "RocCurveDisplay.from_estimator(\n",
    "    grid_tree, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"DecisionTree\"\n",
    ")\n",
    "RocCurveDisplay.from_estimator(\n",
    "    grid_svc, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"SVC\"\n",
    ")\n",
    "RocCurveDisplay.from_estimator(\n",
    "    grid_forest, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"RandomForest\"\n",
    ")\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), label=\"baseline\")\n",
    "ax.set_title(\"Roc Auc curve comparison\")\n",
    "plt.savefig(\"images/roc_curve_comparison.png\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Roc Curve Logistic Regression\n",
    "fig, ax = plt.subplots()\n",
    "PrecisionRecallDisplay.from_estimator(\n",
    "    grid_lr, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"LogisticRegression\"\n",
    ")\n",
    "PrecisionRecallDisplay.from_estimator(\n",
    "    grid_knn, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"KNN\"\n",
    ")\n",
    "PrecisionRecallDisplay.from_estimator(\n",
    "    grid_tree, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"DecisionTree\"\n",
    ")\n",
    "PrecisionRecallDisplay.from_estimator(\n",
    "    grid_svc, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"SVC\"\n",
    ")\n",
    "PrecisionRecallDisplay.from_estimator(\n",
    "    grid_forest, X_test, y_test, pos_label=\"yes\", ax=ax, name=\"RandomForest\"\n",
    ")\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), label=\"baseline\")\n",
    "ax.set_title(\"Precision Recall curve comparison\")\n",
    "plt.savefig(\"images/pr_curve_comparison.png\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logistic_importance = {\n",
    "    \"Feature\": get_feature_names(grid_lr),\n",
    "    \"Importance - Coef\": np.abs(grid_lr.best_estimator_.named_steps[\"lr\"].coef_[0]),\n",
    "}\n",
    "logistic_importance_df = pd.DataFrame(logistic_importance)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logistic_importance_df = logistic_importance_df.sort_values(\n",
    "    \"Importance - Coef\", ascending=False\n",
    ")\n",
    "fig = px.bar(\n",
    "    logistic_importance_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features LogisticRegression\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/logistic_regression_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Logistic Regression confusion matrix\n",
    "cm = confusion_matrix(y_test, grid_lr.best_estimator_.predict(X_test))\n",
    "fig = ConfusionMatrixDisplay(cm, display_labels=grid_lr.best_estimator_.classes_)\n",
    "fig.plot()\n",
    "fig.ax_.set_title(\"LogisticRegression Confusion Matrix\")\n",
    "plt.savefig(\"images/logistic_regression_cm.png\", dpi=100)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tree_importance = {\n",
    "    \"Feature\": get_feature_names(grid_tree),\n",
    "    \"Importance\": np.abs(\n",
    "        grid_tree.best_estimator_.named_steps[\"tree\"].feature_importances_\n",
    "    ),\n",
    "}\n",
    "tree_importance_df = pd.DataFrame(tree_importance)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tree_importance_df = tree_importance_df.sort_values(\"Importance\", ascending=False)\n",
    "fig = px.bar(\n",
    "    tree_importance_df[:10],\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features DecisionTree\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/decision_tree_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DecisionTree confusion matrix\n",
    "cm = confusion_matrix(y_test, grid_tree.best_estimator_.predict(X_test))\n",
    "fig = ConfusionMatrixDisplay(cm, display_labels=grid_tree.best_estimator_.classes_)\n",
    "fig.plot()\n",
    "fig.ax_.set_title(\"DecisionTree Confusion Matrix\")\n",
    "plt.savefig(\"images/tree_cm.png\", dpi=100)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# KNN confusion matrix\n",
    "cm = confusion_matrix(y_test, grid_knn.best_estimator_.predict(X_test))\n",
    "fig = ConfusionMatrixDisplay(cm, display_labels=grid_knn.best_estimator_.classes_)\n",
    "fig.plot()\n",
    "fig.ax_.set_title(\"KNN Confusion Matrix\")\n",
    "plt.savefig(\"images/knn_cm.png\", dpi=100)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# svc confusion matrix\n",
    "cm = confusion_matrix(y_test, grid_svc.best_estimator_.predict(X_test))\n",
    "fig = ConfusionMatrixDisplay(cm, display_labels=grid_svc.best_estimator_.classes_)\n",
    "fig.plot()\n",
    "fig.ax_.set_title(\"SVC Confusion Matrix\")\n",
    "plt.savefig(\"images/svc_cm.png\", dpi=100)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forest_importance = {\n",
    "    \"Feature\": get_feature_names(grid_forest),\n",
    "    \"Importance\": np.abs(\n",
    "        grid_forest.best_estimator_.named_steps[\"forest\"].feature_importances_\n",
    "    ),\n",
    "}\n",
    "forest_importance_df = pd.DataFrame(forest_importance)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forest_importance_df = forest_importance_df.sort_values(\"Importance\", ascending=False)\n",
    "fig = px.bar(\n",
    "    forest_importance_df[:10],\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features RandomForest\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/forest_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RandomForest confusion matrix\n",
    "cm = confusion_matrix(y_test, grid_forest.best_estimator_.predict(X_test))\n",
    "fig = ConfusionMatrixDisplay(cm, display_labels=grid_forest.best_estimator_.classes_)\n",
    "fig.plot()\n",
    "fig.ax_.set_title(\"RandomForest Confusion Matrix\")\n",
    "plt.savefig(\"images/forest_cm.png\", dpi=100)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
